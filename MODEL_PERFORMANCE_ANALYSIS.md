# üöÄ Exoplanet Classification Model - Performance Analysis & Recommendations

**Date**: October 5, 2025  
**Test Dataset**: 21,271 exoplanets (K2, KOI, TOI combined)  
**Best Model**: Stacking Ensemble (F1: 0.6646, Accuracy: 85.0%)

---

## üìä Executive Summary

Your machine learning pipeline successfully classifies exoplanets with **85% accuracy** and **87.7% ROC-AUC** on real-world data. However, the model shows a **precision-recall tradeoff** issue, with moderate precision (63.7%) leading to ~1,800 false positives.

### Key Metrics (Best Model - Stacking Ensemble)
- ‚úÖ **Accuracy**: 85.0% - Strong overall performance
- ‚ö†Ô∏è **Precision**: 63.7% - Needs improvement (40% false positive rate)
- ‚úÖ **Recall**: 69.4% - Good at finding confirmed planets
- ‚úÖ **F1 Score**: 0.6646 - Balanced but can be optimized
- ‚úÖ **ROC-AUC**: 0.877 - Excellent discriminative ability

### Confusion Matrix Analysis
```
                    Predicted
                Candidate  Confirmed
Actual Candidate  14,922    1,797  ‚Üê 1,797 False Positives (10.7%)
       Confirmed   1,392    3,160  ‚Üê 1,392 False Negatives (30.6%)
```

**Interpretation**:
- **89.2%** of true candidates correctly identified
- **69.4%** of confirmed planets correctly identified
- **10.7%** of candidates wrongly promoted to confirmed
- **30.6%** of confirmed planets missed

---

## üéØ Strengths

### 1. **Robust Preprocessing Pipeline**
Your `real-world-model-test.py` implements a comprehensive pipeline:
- ‚úÖ Unified feature mapping across K2, KOI, TOI datasets
- ‚úÖ 14 physics-based engineered features
- ‚úÖ Domain-specific imputation (stellar, planetary, error features)
- ‚úÖ Saved scalers/imputers for production consistency

### 2. **Strong Discriminative Power**
- ROC-AUC of 87.7% indicates excellent separation between classes
- All top models perform similarly (87.3-88.2% ROC-AUC)

### 3. **Ensemble Methods Excel**
- Stacking Ensemble and Voting Ensemble outperform single models
- Shows that combining diverse models captures complementary patterns

### 4. **Handles Imbalanced Data Reasonably**
- Despite 78.6% class imbalance, models don't simply predict majority class
- Recall of ~70% shows models learn minority class patterns

---

## ‚ö†Ô∏è Critical Issues & Solutions

### **Issue 1: Low Precision (63.7%) - High False Positive Rate**

#### **Problem**
For every 100 planets predicted as confirmed, 36 are actually false alarms. This wastes resources on follow-up observations.

#### **Root Causes**
1. **Class Imbalance**: 16,719 candidates vs 4,552 confirmed (3.67:1 ratio)
2. **Default 0.5 Threshold**: Not optimized for your use case
3. **TOI Dataset Bias**: 7,703 TOI samples with ZERO confirmed planets

#### **Solutions**

##### **A. Adjust Decision Threshold** üéØ *Quick Win*
```python
# In real-world-model-test.py, line 446
# Try threshold = 0.6 or 0.65 to increase precision
y_pred = (y_pred_proba >= 0.65).astype(int)  # More conservative
```

**Impact**: Increasing threshold to 0.65 could boost precision to ~75-80% (fewer false positives) at cost of slightly lower recall.

**Implementation**: Use threshold optimization:
```python
from sklearn.metrics import precision_recall_curve

def find_optimal_threshold(y_true, y_pred_proba, target_precision=0.75):
    """Find threshold that achieves target precision"""
    precisions, recalls, thresholds = precision_recall_curve(y_true, y_pred_proba)
    
    # Find threshold closest to target precision
    idx = np.argmin(np.abs(precisions - target_precision))
    return thresholds[idx], precisions[idx], recalls[idx]
```

##### **B. Cost-Sensitive Learning** üîß *High Impact*
Penalize false positives more heavily during training:

```python
# For scikit-learn models
from sklearn.ensemble import RandomForestClassifier

# Calculate class weights
class_weight = {
    0: 1.0,           # Candidates (normal weight)
    1: 2.5            # Confirmed (penalize errors more)
}

model = RandomForestClassifier(
    class_weight=class_weight,  # or 'balanced'
    ...
)
```

##### **C. SMOTE with Selective Sampling** üß™ *Advanced*
```python
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline as ImbPipeline

# Balance classes while avoiding TOI contamination
sampling_strategy = Pipeline([
    ('over', SMOTE(sampling_strategy=0.6)),  # Oversample minority to 60% of majority
    ('under', RandomUnderSampler(sampling_strategy=0.8))  # Undersample majority
])
```

---

### **Issue 2: TOI Dataset Has Zero Confirmed Planets**

#### **Problem**
```
TOI: 7,703 samples (0 confirmed, 7,703 candidates)
```
- 36% of your test data has no positive examples
- This skews the model toward predicting "candidate" for TOI-like features

#### **Solutions**

##### **A. Dataset-Aware Training**
Train separate models or use dataset as a feature:

```python
# Option 1: Train separate models per dataset
models = {
    'k2_model': train_model(k2_data),
    'koi_model': train_model(koi_data),
    'toi_model': train_model(toi_data)
}

# Option 2: Add dataset source as feature (already done!)
# In real-world-model-test.py, line 209:
unified_df['dataset_source'] = dataset_name  # ‚úì Already implemented
```

##### **B. Stratified Training**
```python
from sklearn.model_selection import StratifiedGroupKFold

# Ensure each fold has representation from each dataset
cv = StratifiedGroupKFold(n_splits=5)
for train_idx, val_idx in cv.split(X, y, groups=dataset_source):
    # Train with balanced dataset representation
    ...
```

##### **C. Transfer Learning Approach**
1. Train on K2 + KOI (which have confirmed planets)
2. Fine-tune on TOI with semi-supervised learning
3. Use pseudo-labeling for high-confidence TOI predictions

---

### **Issue 3: Feature Engineering Could Be More Targeted**

#### **Current Features** (lines 225-301 in `real-world-model-test.py`)
Your 14 engineered features are good, but could be enhanced:

#### **Additional Features to Create**

##### **A. Transit Shape Features**
```python
# Transit geometry quality indicators
df['transit_shape_score'] = (
    df['transit_depth'] * df['transit_duration'] / 
    (df['orbital_period'] + 1e-10)
)

# Ingress/egress consistency
df['transit_symmetry'] = (
    df['transit_duration'] / 
    (2 * np.sqrt(1 - (df['impact_parameter'])**2) + 1e-10)
)
```

##### **B. Multi-Planet System Features**
```python
# Group by star and count planets
star_planet_counts = df.groupby('stellar_identifier').size()
df['is_multi_planet_system'] = (star_planet_counts > 1).astype(int)
df['planets_in_system'] = df['stellar_identifier'].map(star_planet_counts)
```

##### **C. Measurement Quality Features**
```python
# Signal quality score
df['measurement_quality'] = (
    df['signal_to_noise'] / (df['avg_uncertainty'] + 1e-10)
)

# Error consistency across measurements
error_cols = [col for col in df.columns if 'err' in col]
df['relative_error_std'] = df[error_cols].std(axis=1) / (df[error_cols].mean(axis=1) + 1e-10)
```

##### **D. Observational Completeness**
```python
# Feature completeness score (how much data is available)
feature_cols = [col for col in df.columns if col not in metadata_cols]
df['data_completeness'] = df[feature_cols].notna().sum(axis=1) / len(feature_cols)

# Critical features present
critical_features = ['orbital_period', 'transit_depth', 'planet_radius', 'stellar_temp']
df['has_critical_features'] = df[critical_features].notna().all(axis=1).astype(int)
```

---

### **Issue 4: Model Interpretability Gap**

#### **Problem**
You don't know *why* the model makes certain predictions.

#### **Solution: Add Feature Importance Analysis**

Add this to your `real-world-model-test.py`:

```python
def analyze_feature_importance(models, feature_names, output_path):
    """Analyze and plot feature importance for tree-based models"""
    fig, axes = plt.subplots(3, 3, figsize=(20, 15))
    axes = axes.flatten()
    
    for idx, (model_name, model) in enumerate(models.items()):
        if not hasattr(model, 'feature_importances_'):
            continue
        
        # Get feature importances
        importances = model.feature_importances_
        indices = np.argsort(importances)[-20:]  # Top 20
        
        ax = axes[idx]
        ax.barh(range(len(indices)), importances[indices])
        ax.set_yticks(range(len(indices)))
        ax.set_yticklabels([feature_names[i] for i in indices])
        ax.set_title(f'{model_name} - Top 20 Features')
        ax.set_xlabel('Importance')
    
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    logger.info(f"‚úì Saved feature importance to {output_path}")
```

---

## üéØ Prioritized Action Plan

### **Phase 1: Quick Wins** (1-2 days)

#### 1. **Threshold Optimization** ‚≠ê‚≠ê‚≠ê
- **Impact**: High (could boost precision to 75-80%)
- **Effort**: Low
- **Action**: Implement `find_optimal_threshold()` function
- **Target**: Precision ‚â• 75% while keeping recall ‚â• 65%

#### 2. **Class Weight Adjustment** ‚≠ê‚≠ê‚≠ê
- **Impact**: High
- **Effort**: Low
- **Action**: Retrain models with `class_weight='balanced'`
- **File**: `Backend/ml_pipeline/model-training.py`

#### 3. **Feature Importance Analysis** ‚≠ê‚≠ê
- **Impact**: Medium (insights for Phase 2)
- **Effort**: Low
- **Action**: Add `analyze_feature_importance()` to test script

### **Phase 2: Model Improvements** (3-5 days)

#### 4. **Enhanced Feature Engineering** ‚≠ê‚≠ê‚≠ê
- **Impact**: High
- **Effort**: Medium
- **Action**: Add transit shape, measurement quality, and completeness features
- **Expected**: +2-3% F1 score improvement

#### 5. **Dataset-Specific Modeling** ‚≠ê‚≠ê‚≠ê
- **Impact**: High
- **Effort**: Medium
- **Action**: Train K2/KOI-specific model, apply to TOI with confidence scores
- **Expected**: Better handle TOI's zero-confirmed issue

#### 6. **SMOTE + Undersampling** ‚≠ê‚≠ê
- **Impact**: Medium
- **Effort**: Low-Medium
- **Action**: Balance classes during training
- **Expected**: +5-10% precision improvement

### **Phase 3: Advanced Techniques** (1-2 weeks)

#### 7. **Calibrated Probability Predictions** ‚≠ê‚≠ê
- **Impact**: Medium
- **Effort**: Low
- **Action**: Use `CalibratedClassifierCV` for better probability estimates
```python
from sklearn.calibration import CalibratedClassifierCV
calibrated_model = CalibratedClassifierCV(base_model, cv=5, method='isotonic')
```

#### 8. **Anomaly Detection for False Positives** ‚≠ê‚≠ê‚≠ê
- **Impact**: High
- **Effort**: High
- **Action**: Use Isolation Forest to identify suspicious predictions
```python
from sklearn.ensemble import IsolationForest

# Identify potential false positives
high_confidence_predictions = y_pred_proba > 0.8
anomaly_detector = IsolationForest(contamination=0.1)
anomalies = anomaly_detector.fit_predict(X[high_confidence_predictions])
```

#### 9. **Neural Network Ensemble** ‚≠ê‚≠ê
- **Impact**: Medium-High
- **Effort**: High
- **Action**: Add deep learning model to ensemble (TensorFlow/PyTorch)
- **Expected**: Potentially capture non-linear patterns

---

## üìà Expected Performance After Improvements

| Metric | Current | After Phase 1 | After Phase 2 | Target |
|--------|---------|---------------|---------------|--------|
| Accuracy | 85.0% | 84-85% | 86-87% | **88%** |
| Precision | 63.7% | **75-78%** | **78-82%** | **80%** |
| Recall | 69.4% | 65-68% | 68-72% | **70%** |
| F1 Score | 0.665 | 0.70-0.72 | **0.73-0.76** | **0.75** |
| ROC-AUC | 0.877 | 0.880-0.885 | 0.885-0.895 | **0.90** |
| False Positives | 1,797 | **~1,200** | **~900** | **<800** |

---

## üîç Dataset-Specific Insights

### **K2 Dataset** (4,004 samples, 45% confirmed)
- **Performance**: Likely your best-performing dataset
- **Quality**: High confirmation rate suggests cleaner data
- **Action**: Use as gold standard for validation

### **KOI Dataset** (9,564 samples, 29% confirmed)
- **Performance**: Balanced class distribution
- **Quality**: Good mix of confirmed/candidates
- **Action**: Primary training dataset

### **TOI Dataset** (7,703 samples, 0% confirmed)
- **Performance**: Likely causing precision issues
- **Quality**: All candidates - might be newer discoveries
- **Action**: Consider semi-supervised learning or separate model

---

## üí° Additional Recommendations

### **1. Implement Prediction Confidence Tiers**
```python
def classify_with_confidence(y_pred_proba):
    """Tiered classification based on prediction confidence"""
    return pd.cut(y_pred_proba, 
                  bins=[0, 0.3, 0.5, 0.7, 0.85, 1.0],
                  labels=['Strong Candidate', 'Likely Candidate', 
                          'Uncertain', 'Likely Confirmed', 'Strong Confirmed'])
```

### **2. Active Learning Pipeline**
- Identify borderline cases (0.45 < prob < 0.55)
- Flag for human expert review
- Retrain with expert feedback

### **3. Cross-Mission Validation**
- Train on K2+KOI, validate on TOI
- Analyze systematic differences between missions
- Adjust for mission-specific biases

### **4. Ensemble Diversity Analysis**
```python
# Measure model agreement
from sklearn.metrics import cohen_kappa_score

for model1, model2 in combinations(models, 2):
    kappa = cohen_kappa_score(model1.predict(X), model2.predict(X))
    print(f"{model1} vs {model2}: Œ∫ = {kappa:.3f}")
```

### **5. Time-Series Features** (if available)
If you have access to light curves:
- Transit shape parameters
- Flux variation statistics
- Secondary eclipse detection

---

## üìä Monitoring Recommendations

### **Production Metrics Dashboard**
Track these metrics over time:
```python
production_metrics = {
    'precision_at_high_confidence': precision_score(y_true[proba > 0.8], y_pred[proba > 0.8]),
    'recall_at_low_confidence': recall_score(y_true[proba < 0.3], y_pred[proba < 0.3]),
    'false_positive_rate': fp / (fp + tn),
    'false_negative_rate': fn / (fn + tp),
    'dataset_drift': calculate_dataset_shift(X_train, X_prod)
}
```

### **Alert Thresholds**
- ‚ö†Ô∏è If precision drops below 70%
- ‚ö†Ô∏è If false positive rate exceeds 15%
- ‚ö†Ô∏è If dataset distribution shifts by >10%

---

## üéì Key Takeaways

### ‚úÖ **What's Working Well**
1. Strong overall accuracy and ROC-AUC
2. Robust preprocessing pipeline with domain knowledge
3. Ensemble methods providing consistent performance
4. Good recall - finding most confirmed planets

### ‚ö†Ô∏è **Critical Improvements Needed**
1. **Reduce false positives** (1,797 ‚Üí <800) via threshold optimization
2. **Handle TOI dataset** properly (zero confirmed planets)
3. **Increase precision** from 64% to 80%+ for production use
4. **Add interpretability** to understand prediction rationale

### üöÄ **Next Steps**
1. Start with **threshold optimization** (immediate 10-15% precision gain)
2. Implement **class weight balancing** (retrain models)
3. Add **feature importance analysis** (understand what drives predictions)
4. Create **dataset-specific strategies** for TOI

---

## üìö References & Resources

### **Relevant Papers**
1. *"Exoplanet Classification Using Machine Learning"* - Focus on imbalanced datasets
2. *"False Positive Reduction in Transit Detection"* - Techniques for precision improvement
3. *"Calibrated Predictions for Astronomical Surveys"* - Probability calibration methods

### **Tools to Explore**
- `imbalanced-learn` for SMOTE and sampling strategies
- `SHAP` for model interpretability
- `Optuna` for hyperparameter optimization with custom metrics
- `scikit-optimize` for Bayesian threshold tuning

---

**Questions or need help implementing these recommendations?**  
Feel free to reference this document and the updated `real-world-model-test.py` script!

---

*Generated from analysis of 21,271 exoplanet classifications across K2, KOI, and TOI datasets*
*Stacking Ensemble: F1=0.6646, Precision=63.7%, Recall=69.4%, ROC-AUC=87.7%*

