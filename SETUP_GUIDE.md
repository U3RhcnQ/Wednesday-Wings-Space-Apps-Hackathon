# Quick Setup Guide for Monorepo Structure
# NASA Space Apps Challenge 2025

## ğŸš€ Quick File Organization

### **Step 1: Create the basic structure**
```bash
# From your project root
mkdir -p backend/{ml_pipeline,sanitization,api,data/{raw,sanitized,processed},models,metadata,plots,logs,datasets,cleaned_datasets}
mkdir -p frontend/{src,public}
mkdir -p shared
mkdir -p scripts
mkdir -p docs
```

### **Step 2: Move generated files to correct locations**

**Core ML Pipeline files** â†’ `backend/ml_pipeline/`:
- `data-acquisition.py`
- `preprocessing.py` 
- `enhanced-preprocessing.py`
- `model-training.py`
- `enhanced-inference.py`
- `run-monorepo-pipeline.py` (the new main script)

**Your existing sanitization scripts** â†’ `backend/sanitization/`:
- `koi_data_sanitizer.py`
- `toi_data_sanitizer.py`
- `k2_data_sanitizer.py`
- `run_all_sanitizers.py`

**Configuration files** â†’ project root:
- `requirements.txt`
- `README.md`
- `MONOREPO_STRUCTURE.md`

### **Step 3: Update script paths**
The new `run-monorepo-pipeline.py` automatically handles all path management, so your existing scripts don't need changes!

## ğŸ“‹ Execution Commands

### **From project root:**
```bash
# Setup environment
cd backend/
pip install -r requirements.txt

# Run the complete pipeline
cd ml_pipeline/
python run_monorepo_pipeline.py
```

### **Individual stages (if needed):**
```bash
# From backend/ml_pipeline/
python data_acquisition.py

# From backend/sanitization/ 
python run_all_sanitizers.py

# From backend/ml_pipeline/
python enhanced_preprocessing.py
python model_training.py
python enhanced_inference.py
```

## ğŸ¯ Key Benefits of This Structure

âœ… **Clean Separation**: Frontend and backend are completely separate
âœ… **Preserved Scripts**: Your existing sanitization scripts work unchanged
âœ… **Scalable**: Easy to add API endpoints, web UI, Docker containers
âœ… **Path Management**: All file paths handled automatically
âœ… **Data Safety**: Original data preserved in `backend/data/raw/`

## ğŸ“‚ Where Everything Goes

```
your-project/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ ml_pipeline/           # â† Put our generated ML files here
â”‚   â”‚   â”œâ”€â”€ data_acquisition.py
â”‚   â”‚   â”œâ”€â”€ enhanced_preprocessing.py
â”‚   â”‚   â”œâ”€â”€ model_training.py
â”‚   â”‚   â”œâ”€â”€ enhanced_inference.py
â”‚   â”‚   â””â”€â”€ run_monorepo_pipeline.py
â”‚   â”‚
â”‚   â”œâ”€â”€ sanitization/          # â† Put your existing scripts here  
â”‚   â”‚   â”œâ”€â”€ koi_data_sanitizer.py
â”‚   â”‚   â”œâ”€â”€ toi_data_sanitizer.py
â”‚   â”‚   â”œâ”€â”€ k2_data_sanitizer.py
â”‚   â”‚   â””â”€â”€ run_all_sanitizers.py
â”‚   â”‚
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ raw/               # â† Original data backups
â”‚   â”‚   â”œâ”€â”€ sanitized/         # â† Your cleaned data
â”‚   â”‚   â””â”€â”€ processed/         # â† Final ML features
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                # â† Trained models
â”‚   â”œâ”€â”€ plots/                 # â† Generated visualizations
â”‚   â””â”€â”€ metadata/              # â† Execution logs
â”‚
â”œâ”€â”€ frontend/                  # â† Future web interface
â”‚   â”œâ”€â”€ src/
â”‚   â””â”€â”€ public/
â”‚
â”œâ”€â”€ requirements.txt           # â† Python dependencies
â””â”€â”€ README.md                  # â† Project documentation
```

## ğŸ”§ Running the Enhanced Pipeline

The new monorepo pipeline will:

1. **ğŸ” Auto-detect** your sanitization scripts
2. **ğŸ“¥ Download** NASA data to `backend/datasets/`
3. **ğŸ§¹ Run** your sanitizers â†’ outputs to `backend/cleaned_datasets/`
4. **ğŸ”„ Integrate** cleaned data into unified format
5. **âš–ï¸ Apply** SMOTE balancing and feature scaling
6. **ğŸ¤– Train** H100-optimized models
7. **ğŸ“Š Generate** comprehensive performance analysis
8. **ğŸ’¾ Save** everything in organized structure

**Command:**
```bash
cd backend/ml_pipeline/
python run_monorepo_pipeline.py
```

That's it! The script handles all path management, integration, and data flow automatically while preserving your existing workflow.