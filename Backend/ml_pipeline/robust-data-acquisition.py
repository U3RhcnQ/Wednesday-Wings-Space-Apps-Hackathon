# Fixed Robust Data Acquisition with Working URLs
# NASA Space Apps Challenge 2025
# Uses your proven working NASA API endpoints

import sys
import os
from pathlib import Path

# Add project paths
current_dir = Path(__file__).parent
backend_dir = current_dir.parent
project_root = backend_dir.parent

sys.path.extend([
    str(backend_dir),
    str(backend_dir / 'config'),
    str(backend_dir / 'utils'),
    str(backend_dir / 'ml_pipeline')
])

import numpy as np
import pandas as pd
import requests
import warnings
import json
import hashlib
from datetime import datetime

warnings.filterwarnings('ignore')

# Import path configuration (create if not exists)
try:
    from config.paths import PROJECT_PATHS, ensure_dir

    PATHS_CONFIGURED = True
except ImportError:
    PATHS_CONFIGURED = False
    PROJECT_PATHS = {
        'data_raw': backend_dir / 'data' / 'raw',
        'data_sanitized': backend_dir / 'data' / 'sanitized',
        'data_processed': backend_dir / 'data' / 'processed',
        'metadata': backend_dir / 'metadata',
        'ml_metadata': current_dir / 'metadata',
        'logs': backend_dir / 'logs'
    }


    def ensure_dir(name):
        path = PROJECT_PATHS.get(name)
        if path:
            path.mkdir(parents=True, exist_ok=True)
            return path
        return None


class FixedDataAcquisition:
    """
    Fixed data acquisition using your proven working NASA API URLs
    """

    def __init__(self):
        # Ensure all required directories exist
        for dir_name in ['datasets', 'data_raw', 'metadata', 'ml_metadata', 'logs']:
            ensure_dir(dir_name)

        self.paths = PROJECT_PATHS
        self.datasets = {}

        # Your proven working dataset configurations
        self.dataset_configs = {
            "koi": {
                "name": "kepler_koi",
                "description": "Kepler KOI - Cumulative table",
                "url": "https://exoplanetarchive.ipac.caltech.edu/TAP/sync?query=select+*+from+cumulative&format=csv",
                "disposition_col": "koi_disposition",
                "filename_base": "koi"
            },
            "toi": {
                "name": "tess_toi",
                "description": "TESS TOI table",
                "url": "https://exoplanetarchive.ipac.caltech.edu/TAP/sync?query=select+*+from+toi&format=csv",
                "disposition_col": "tfopwg_disp",
                "filename_base": "toi"
            },
            "k2": {
                "name": "k2_candidates",
                "description": "K2 Planets and Candidates",
                "url": "https://exoplanetarchive.ipac.caltech.edu/TAP/sync?query=select+*+from+k2pandc&format=csv",
                "disposition_col": "disposition",
                "filename_base": "k2"
            }
        }

        self.metadata = {
            'pipeline_version': '2.1.1',
            'module': 'fixed_data_acquisition',
            'creation_date': datetime.now().isoformat(),
            'paths_configured': PATHS_CONFIGURED,
            'backend_root': str(backend_dir),
            'working_directory': str(current_dir),
            'api_urls_tested': True,
            'data_sources': [],
            'processing_steps': []
        }

        self.log_file = self.paths['logs'] / f"data_acquisition_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

        print("=" * 80)
        print("üöÄ FIXED EXOPLANET DATA ACQUISITION")
        print("NASA Space Apps Challenge 2025")
        print("=" * 80)
        print(f"Pipeline Version: {self.metadata['pipeline_version']}")
        print(f"Backend Root: {backend_dir}")
        print(f"Using Proven Working NASA API URLs")
        print(f"Log File: {self.log_file}")
        print("=" * 80)

        self.log("Fixed data acquisition initialized with working URLs")

    def log(self, message, level="INFO"):
        """Log messages to file and console"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        log_entry = f"[{timestamp}] [{level}] {message}"

        try:
            with open(self.log_file, 'a') as f:
                f.write(log_entry + '\\n')
        except:
            pass

        if level == "INFO":
            print(f"üìù {message}")
        elif level == "ERROR":
            print(f"‚ùå {message}")
        elif level == "WARNING":
            print(f"‚ö†Ô∏è  {message}")

    def check_existing_data(self):
        """Check for existing data files"""
        self.log("Checking for existing data files...")

        existing_files = {}

        # Check datasets directory first
        datasets_dir = self.paths['data_raw']
        if datasets_dir.exists():
            for dataset_key, config in self.dataset_configs.items():
                filename_base = config['filename_base']

                # Check for exact filename match
                expected_file = datasets_dir / f"{filename_base}.csv"
                if expected_file.exists():
                    file_info = {
                        'path': str(expected_file),
                        'size_mb': expected_file.stat().st_size / (1024 * 1024),
                        'modified': datetime.fromtimestamp(expected_file.stat().st_mtime).isoformat(),
                        'dataset_key': dataset_key
                    }
                    existing_files[dataset_key] = file_info
                    self.log(f"Found existing file: {expected_file.name} ({file_info['size_mb']:.2f} MB)")

        return existing_files

    def download_dataset_fixed(self, dataset_key, force_download=False):
        """
        Download dataset using your proven working URL configuration
        """
        config = self.dataset_configs[dataset_key]
        dataset_name = config['name']

        self.log(f"Processing {dataset_name} dataset...")

        # Define file paths
        datasets_file = self.paths['data_raw'] / f"{dataset_name}_raw.csv"

        dataset_metadata = {
            'dataset_key': dataset_key,
            'dataset_name': dataset_name,
            'description': config['description'],
            'disposition_column': config['disposition_col'],
            'api_url': config['url'],
            'datasets_file': str(datasets_file),
            'processing_timestamp': datetime.now().isoformat()
        }

        # Check if file already exists and is valid
        if datasets_file.exists() and not force_download:
            self.log(f"Found existing data: {datasets_file}")
            try:
                df = pd.read_csv(datasets_file)

                if len(df) == 0:
                    self.log("Existing file is empty, will download fresh data", "WARNING")
                    datasets_file.unlink()
                else:
                    self.log(f"Using existing data: {len(df):,} records, {len(df.columns)} columns")

                    # Quick validation and metadata update
                    dataset_metadata.update({
                        'source': 'existing_cache',
                        'total_records': len(df),
                        'total_columns': len(df.columns),
                        'columns': df.columns.tolist()[:20],  # First 20 columns
                        'file_size_bytes': datasets_file.stat().st_size
                    })

                    # Store dataset
                    self.datasets[dataset_name] = {
                        'df': df,
                        'disposition_col': config['disposition_col'],
                        'metadata': dataset_metadata
                    }
                    self.metadata['data_sources'].append(dataset_metadata)

                    return df, str(datasets_file), dataset_metadata

            except Exception as e:
                self.log(f"Error loading existing file: {e}", "ERROR")
                self.log("Will attempt to download fresh data")

        # Download fresh data using your working URL
        self.log(f"Downloading {dataset_name} from NASA Exoplanet Archive...")
        self.log(f"URL: {config['url']}")

        try:
            self.log("Making API request to NASA Exoplanet Archive...")

            # Use your exact working URL with proper headers
            headers = {
                'User-Agent': 'Mozilla/5.0 (compatible; ExoplanetPipeline/2.1.1)',
                'Accept': 'text/csv,application/csv,text/plain',
                'Accept-Encoding': 'gzip, deflate'
            }

            response = requests.get(
                config['url'],
                headers=headers,
                timeout=300,
                stream=True
            )
            response.raise_for_status()

            # Check if we got CSV data
            content_type = response.headers.get('content-type', '').lower()
            if 'html' in content_type:
                # Likely an error page
                self.log(f"Received HTML instead of CSV - API may have returned error", "ERROR")
                self.log(f"Content-Type: {content_type}")
                # Try to read first part of response to see error
                sample_content = response.content[:500].decode('utf-8', errors='ignore')
                self.log(f"Response sample: {sample_content}")
                raise requests.RequestException("API returned HTML error page instead of CSV data")

            # Save to datasets directory (for compatibility with your sanitizers)
            self.log(f"Saving to: {datasets_file}")
            with open(datasets_file, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)

            # Load and validate data
            df = pd.read_csv(datasets_file)

            if len(df) == 0:
                raise ValueError("Downloaded file is empty")

            # Update metadata
            dataset_metadata.update({
                'source': 'nasa_api_download',
                'total_records': len(df),
                'total_columns': len(df.columns),
                'columns': df.columns.tolist()[:20],  # First 20 columns
                'file_size_bytes': datasets_file.stat().st_size,
                'file_hash': self.calculate_hash(datasets_file)
            })

            self.log(f"‚úÖ Download successful: {len(df):,} records, {len(df.columns)} columns")
            self.log(f"File size: {dataset_metadata['file_size_bytes'] / 1024 / 1024:.2f} MB")

            # Analyze disposition column
            self.analyze_disposition_column(df, config['disposition_col'], dataset_metadata)

            # Store dataset
            self.datasets[dataset_name] = {
                'df': df,
                'disposition_col': config['disposition_col'],
                'metadata': dataset_metadata
            }
            self.metadata['data_sources'].append(dataset_metadata)

            return df, str(datasets_file), dataset_metadata

        except requests.HTTPError as e:
            error_msg = f"HTTP error downloading {dataset_name}: {e}"
            self.log(error_msg, "ERROR")
            self.log(f"Response status: {e.response.status_code if e.response else 'Unknown'}")
            if e.response and e.response.content:
                sample_response = e.response.content[:200].decode('utf-8', errors='ignore')
                self.log(f"Error response sample: {sample_response}")
            dataset_metadata['error'] = {'type': 'http_error', 'message': str(e)}

        except requests.RequestException as e:
            error_msg = f"Network error downloading {dataset_name}: {e}"
            self.log(error_msg, "ERROR")
            dataset_metadata['error'] = {'type': 'network_error', 'message': str(e)}

        except Exception as e:
            error_msg = f"Error downloading {dataset_name}: {e}"
            self.log(error_msg, "ERROR")
            dataset_metadata['error'] = {'type': 'general_error', 'message': str(e)}

        # Save error metadata
        error_file = self.paths['ml_metadata'] / f"{dataset_name}_error_metadata.json"
        with open(error_file, 'w') as f:
            json.dump(dataset_metadata, f, indent=4, default=str)

        return None, None, dataset_metadata

    def analyze_disposition_column(self, df, disposition_col, metadata):
        """Analyze the disposition column for planet classification"""
        if disposition_col not in df.columns:
            self.log(f"Warning: Column '{disposition_col}' not found!", "WARNING")

            # Try to find similar columns
            disp_cols = [col for col in df.columns if 'disp' in col.lower()]
            if disp_cols:
                self.log(f"Available disposition columns: {disp_cols}")
                disposition_col = disp_cols[0]
                metadata['disposition_column'] = disposition_col
                metadata['disposition_column_auto_detected'] = True
                self.log(f"Using auto-detected column: {disposition_col}")
            else:
                metadata['disposition_column_found'] = False
                return

        # Analyze class distribution
        class_dist = df[disposition_col].value_counts().to_dict()
        missing_values = int(df[disposition_col].isnull().sum())

        metadata.update({
            'disposition_column_found': True,
            'class_distribution': class_dist,
            'missing_values': missing_values
        })

        self.log(f"Class distribution for {disposition_col}:")
        total = len(df)
        for cls, count in class_dist.items():
            pct = (count / total) * 100
            self.log(f"  ‚Ä¢ {cls}: {count:,} ({pct:.2f}%)")

        if missing_values > 0:
            self.log(f"Missing values: {missing_values:,}")

    def calculate_hash(self, filepath):
        """Calculate SHA256 hash for file integrity"""
        hash_sha256 = hashlib.sha256()
        try:
            with open(filepath, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_sha256.update(chunk)
            return hash_sha256.hexdigest()
        except Exception as e:
            self.log(f"Error calculating hash: {e}", "ERROR")
            return None

    def download_all_datasets(self, force_download=False):
        """Download all datasets using proven working URLs"""
        self.log("Starting batch download with proven working URLs...")

        # Check existing data first
        existing_files = self.check_existing_data()

        successful_downloads = 0

        print("\\n" + "=" * 80)
        print("üì• DOWNLOADING ALL DATASETS (FIXED URLS)")
        print("=" * 80)

        for dataset_key, config in self.dataset_configs.items():
            df, filepath, metadata = self.download_dataset_fixed(dataset_key, force_download)

            if df is not None:
                successful_downloads += 1
                print(f"‚úÖ {config['name']}: {len(df):,} records")
            else:
                print(f"‚ùå {config['name']}: Failed")

        # Save overall metadata
        self.metadata.update({
            'successful_downloads': successful_downloads,
            'total_datasets': len(self.dataset_configs),
            'download_complete_timestamp': datetime.now().isoformat(),
            'existing_files_found': len(existing_files)
        })

        # Save metadata to both locations
        for metadata_dir in [self.paths['metadata'], self.paths['ml_metadata']]:
            summary_path = metadata_dir / "data_acquisition_summary.json"
            with open(summary_path, 'w') as f:
                json.dump(self.metadata, f, indent=4, default=str)
            self.log(f"Summary saved: {summary_path}")

        print("\\n" + "=" * 80)
        print("üìä DATA ACQUISITION SUMMARY")
        print("=" * 80)
        print(f"‚úÖ Successful: {successful_downloads}/{len(self.dataset_configs)} datasets")
        print(f"üìÅ Existing files: {len(existing_files)}")
        print(f"üîó Using proven working NASA API URLs")
        print(f"üìù Log file: {self.log_file}")
        print("=" * 80)

        return successful_downloads > 0

    def generate_statistics(self):
        """Generate comprehensive statistics across all datasets"""
        if not self.datasets:
            self.log("No datasets available for statistics", "WARNING")
            return None

        self.log("Generating combined statistics...")

        stats = {
            'generation_timestamp': datetime.now().isoformat(),
            'total_samples_all_datasets': 0,
            'total_confirmed_planets': 0,
            'dataset_breakdown': {}
        }

        for name, data in self.datasets.items():
            df = data['df']
            disp_col = data['disposition_col']

            # Count confirmed planets using multiple patterns
            confirmed_count = 0
            if disp_col in df.columns:
                confirmed_mask = df[disp_col].astype(str).str.contains(
                    'CONFIRMED|CP|PLANET', case=False, na=False
                )
                confirmed_count = confirmed_mask.sum()

            dataset_stats = {
                'total_samples': len(df),
                'confirmed_planets': int(confirmed_count),
                'planet_percentage': (confirmed_count / len(df)) * 100 if len(df) > 0 else 0,
                'disposition_column': disp_col,
                'columns_count': len(df.columns)
            }

            stats['dataset_breakdown'][name] = dataset_stats
            stats['total_samples_all_datasets'] += len(df)
            stats['total_confirmed_planets'] += confirmed_count

        # Overall statistics
        total_samples = stats['total_samples_all_datasets']
        stats['overall_planet_percentage'] = (
            (stats['total_confirmed_planets'] / total_samples * 100)
            if total_samples > 0 else 0
        )

        # Save statistics
        stats_path = self.paths['metadata'] / "combined_dataset_statistics.json"
        with open(stats_path, 'w') as f:
            json.dump(stats, f, indent=4, default=str)

        self.log("üìä Combined Dataset Statistics:")
        self.log(f"   - Total samples: {stats['total_samples_all_datasets']:,}")
        self.log(f"   - Confirmed planets: {stats['total_confirmed_planets']:,}")
        self.log(f"   - Planet percentage: {stats['overall_planet_percentage']:.2f}%")
        self.log(f"   - Statistics saved: {stats_path}")

        return stats


def main():
    """Main execution function with fixed URLs"""
    print("üåü Starting Fixed Data Acquisition with Working URLs...")

    # Initialize data acquisition
    data_acq = FixedDataAcquisition()

    try:
        # Download all datasets
        success = data_acq.download_all_datasets()

        # Generate statistics if we have data
        if data_acq.datasets:
            data_acq.generate_statistics()

        if success or len(data_acq.datasets) > 0:
            data_acq.log("‚úÖ FIXED DATA ACQUISITION COMPLETED SUCCESSFULLY!")

            print("\\nüéØ NEXT STEPS:")
            print("1. ‚úÖ Data now available in datasets/ directory")
            print("2. üßπ Run sanitization: Your scripts will find the data")
            print("3. üîÑ Continue with preprocessing and training")
            print("4. üöÄ All URLs are now working correctly!")

            return True
        else:
            data_acq.log("‚ö†Ô∏è  No datasets acquired - check network connectivity", "WARNING")
            return False

    except Exception as e:
        data_acq.log(f"Fatal error in fixed data acquisition: {e}", "ERROR")
        import traceback
        data_acq.log(traceback.format_exc(), "ERROR")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 0)  # Always exit 0 to allow pipeline continuation